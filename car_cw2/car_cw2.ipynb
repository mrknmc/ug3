{
 "metadata": {
  "name": "",
  "signature": "sha256:9d6467da2f7a1f74c2b526a0e65030155266a493cabdb588756574b2c7868ef9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Note**: I have been able to complete all the parts of the assignment."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The purpose of this report is to describe the internal structure of the cache simulator built for this assignment and summarise the experiments performed on it. The language of my choice is Python. I tried keeping the code simple and adding useful docstrings and comments. The cache simulator is implemented in a function called `set_associative` in a file called `cache.py`. Since direct-mapped cache is the same as 1-way set-associative function we can use this function for both, given that the arguments are set correctly.\n",
      "\n",
      "Furthermore, I wrote a `main` function which just opens the supplied file and runs the `set_associative` function on it and a helper generator function `parse` which loops over the lines of the file and yields the opcode, the tag, the index and the offset of the instruction. It also resets the file pointer after it exits the file loop."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Internal Structure and Workings of the Simulator\n",
      "\n",
      "## Parsing the file\n",
      "\n",
      "Below is the function `parse` used for parsing the file. As arguments it takes the file used, the size of the offset and the size of the index in bits. Using these sizes we create bitmasks which are later used on the addresses. For each line of the file we get the opcode and the address. Using our offset bitmask we extract the offset and then shift by the length of the offset. We do the same for the index and after we are done the address only contains the tag. At the end we reset the file pointer of `file_` so we can re-parse it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def parse(file_, offset_size, index_size):\n",
      "    offset_mask = pow(2, offset_size) - 1\n",
      "    index_mask = pow(2, index_size) - 1\n",
      "\n",
      "    for line in file_:\n",
      "        op, addr = line.split()\n",
      "        addr = int(addr, 16)\n",
      "\n",
      "        # get offset with a bitmask then shift\n",
      "        offset = addr & offset_mask\n",
      "        addr = addr >> offset_size\n",
      "\n",
      "        # get index with a bitmask then shift\n",
      "        index = addr & index_mask\n",
      "        addr = addr >> index_size\n",
      "\n",
      "        # now only tag left\n",
      "        tag = addr\n",
      "\n",
      "        yield op, tag, index, offset\n",
      "\n",
      "    file_.seek(0)  # reset file pointer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Running the Simulator\n",
      "\n",
      "Below is the function `set_associative` responsible for running the cache simulator. As arguments it takes the file used, the size of the cache in bytes, the size of the cache block in bytes and the set-associativity. We use Python's dictionary data structure to simulate the cache as well as record the number of misses and the total number of operations. We also use the deque data structure from Python 2.7 for maintaining the set of tags at a particular index of the cache.\n",
      "\n",
      "Next, we compute the number of bits required for byte-offseting and the number of bits required for indexing every entry of the cache with the given size. These are then passed into the `parse` function mentioned above. After that, we record every operation in the `total` dictionary and perform the following actions:\n",
      "\n",
      " - If there is no tag saved at a particular index of the cache we have a cold miss and we add a deque with only the corresponding tag into the cache at the given index. The deque has a maximum length equal to the set-associativity argument of this function. Thus there cannot be more elements than the maximum length of the deque and whenever something is appended to the end of the deque the first element is popped. We use this to maintain our LRU policy.\n",
      " - If there already is a tag saved at a particular index of the cache, we have two possibilities:\n",
      "     1. The tag recorded in the cache is not the same as the tag we are currently processing. In this case we have a conflict miss and we add the tag at the end of our deque in the cache. The least recently used tag will be automatically popped from the front of the deque.\n",
      "     2. The tag recorded in the cache is the same as the tag we are currently processing. In this case we move the current tag to the end of the deque. We do this by first removing it from the deque and then appending it at the end. This means that it has become the most recently used tag and the least recently used tag was removed."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def set_associative(file_, size=4096, block=32, sets=1):\n",
      "    cache = {}\n",
      "    total = {'R': 0, 'W': 0}\n",
      "    misses = {'R': 0, 'W': 0}\n",
      "\n",
      "    offset_size = int(log(block, 2))  # e.g. need 5 bits for 32 bytes\n",
      "    index_size = int(log(size / block / float(sets), 2))\n",
      "\n",
      "    for op, tag, index, offset in parse(file_, offset_size, index_size):\n",
      "        total[op] += 1\n",
      "\n",
      "        if index not in cache:\n",
      "            # not in cache - add it\n",
      "            misses[op] += 1\n",
      "            cache[index] = deque([tag], maxlen=sets)\n",
      "        else:\n",
      "            if tag not in cache[index]:\n",
      "                # append to the end, LRU will be popped\n",
      "                misses[op] += 1\n",
      "                cache[index].append(tag)\n",
      "            else:\n",
      "                cache[index].remove(tag)\n",
      "                cache[index].append(tag)\n",
      "\n",
      "    return total, misses"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Experiments and a Critical Summary of the Results"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I ran experiments on both files gcc_memref.out and mcf_memref.out with cache size varying from 4KB to 64KB and set associativity varying from 1 to 16, as per the requirements of the assignment. To repeat the experiments just run `cache.py` with appropriate parameters (more information is provided in the readme file).\n",
      "\n",
      "The results of these experiments can be best seen in figure 1 in the appendix (last page). Furthermore, a bar chart of the results for the trace file gcc_memref.out is displayed in figure 2 in the appendix (similar pattern can be observed for mcf_memref.out).\n",
      "\n",
      "From the results, we can observe that the miss rate gets lower as we increase the cache size and/or the set associativity. This is understandable as increasing the size of the cache means we can have more indices and are able to store more data in the cache, hence we will reduce the number of conflict misses. Moreover, increased set associativity means that we can store blocks of data in the cache that have the same index but different tags. This also reduces conflict misses.\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}